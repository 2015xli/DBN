{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8 -7 -6 -5]\n",
      " [-4 -3 -2 -1]\n",
      " [ 0  1  2  3]\n",
      " [ 4  5  6  7]]\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 2 3]\n",
      " [4 5 6 7]]\n",
      "[[  3.35350130e-04   9.11051194e-04   2.47262316e-03   6.69285092e-03]\n",
      " [  1.79862100e-02   4.74258732e-02   1.19202922e-01   2.68941421e-01]\n",
      " [  5.00000000e-01   7.31058579e-01   8.80797078e-01   9.52574127e-01]\n",
      " [  9.82013790e-01   9.93307149e-01   9.97527377e-01   9.99088949e-01]]\n",
      "[[-0.99999977 -0.99999834 -0.99998771 -0.9999092 ]\n",
      " [-0.9993293  -0.99505475 -0.96402758 -0.76159416]\n",
      " [ 0.          0.76159416  0.96402758  0.99505475]\n",
      " [ 0.9993293   0.9999092   0.99998771  0.99999834]]\n",
      "[[  1.93367168e-07   5.25626458e-07   1.42880085e-06   3.88388338e-06]\n",
      " [  1.05574896e-05   2.86982322e-05   7.80098831e-05   2.12052848e-04]\n",
      " [  5.76419403e-04   1.56687039e-03   4.25919530e-03   1.15776932e-02]\n",
      " [  3.14714330e-02   8.55482245e-02   2.32544184e-01   6.32120630e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def linear(x):\n",
    "    return x\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - x.max())\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def g_relu(x):\n",
    "    return 1 * (x > 0)\n",
    "def g_sigmoid(x):\n",
    "    return (1 - x) * x\n",
    "def g_tanh(x):\n",
    "    return 1 - x*x\n",
    "def g_linear(x):\n",
    "    return 1 * (x==x)\n",
    "def g_softmax(x):\n",
    "    dx_ds = np.diag(x) - np.dot(x, x.T)\n",
    "    return dx_ds.sum(axis=0).reshape(-1, 1) \n",
    "\n",
    "class Activation:\n",
    "    \n",
    "    types = (\"LINEAR\", \"RELU\", \"SIGMOID\", \"TANH\", \"SOFTMAX\")\n",
    "    \n",
    "    def __init__(self, acti):\n",
    "        funcs = {\n",
    "            \"TANH\" : tanh,\n",
    "            \"SIGMOID\" : sigmoid,\n",
    "            \"RELU\" : relu,\n",
    "            \"LINEAR\" : linear,\n",
    "            \"SOFTMAX\" : softmax\n",
    "        }\n",
    "\n",
    "        grads = {\n",
    "            \"TANH\" : g_tanh,\n",
    "            \"SIGMOID\" : g_sigmoid,\n",
    "            \"RELU\" : g_relu,\n",
    "            \"LINEAR\" : g_linear,\n",
    "            \"SOFTMAX\" : g_softmax\n",
    "        }\n",
    "        self.acti = acti\n",
    "        self.func = funcs[acti]\n",
    "        self.grad = grads[acti]\n",
    "        \n",
    "        return       \n",
    "    \n",
    "    def __str__(self):\n",
    "        s = \"\\nActivation:\" + self.acti\n",
    "        return s\n",
    "\n",
    "    def __test__():\n",
    "       a = np.arange(-8, 8).reshape(4,4)\n",
    "       for i in Activation.types:\n",
    "           act = Activation(i)\n",
    "           b = act.func(a)\n",
    "           print( str(b) )\n",
    "\n",
    "if __name__ == \"__main__\" and '__file__' not in globals():\n",
    "    Activation.__test__()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
